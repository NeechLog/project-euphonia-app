{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/project-euphonia-app/blob/main/training_colabs/Project_Euphonia_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "xbg9ycVRmiJC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_vPgZcbdRwg"
      },
      "source": [
        "# Fine-Tuning ASR opensource models for Project Euphonia App\n",
        "\n",
        "This notebook demonstrates how to fine-tune the Whisper model for automatic speech recognition (ASR) using data from Google Firebase storage, specifically tailored for Project Euphonia.\n",
        "\n",
        "This notebook is meant to run in Google Colab. To run as Jupyter notebook, access to Google Cloud / Firebase storage needs to be changed (you can manually download the recordings via `gsutil`).\n",
        "\n",
        "**Note:** This setup provides a minimal training configuration, primarily for end-to-end demonstration. Further hyperparameter tuning is recommended for improved results.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Google Cloud project with Firebase storage containing audio data and transcriptions.\n",
        "- Access to a Google Colab environment with a GPU (L4 sufficient for smaller model sizes, A100 needed for large models).\n",
        "- Recorded audio files provided by Euphonia App."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElY87WI4DN6V"
      },
      "source": [
        "## Allow to access Google Cloud storage\n",
        "\n",
        "This cell authenticates your Google Colab session with your Google Cloud account, allowing access to Google Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOQ3jMe5DMn0"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9e4g78MdQDd"
      },
      "source": [
        "## Imports\n",
        "\n",
        "This cell installs or upgrades the necessary Python libraries, including datasets, transformers, evaluate, and others required for audio processing and model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8MeP7B5BjSs"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet pip\n",
        "!pip install --upgrade --quiet datasets[audio] transformers accelerate evaluate jiwer tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZp3apEwmeeP"
      },
      "source": [
        "This cell imports the required Python modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvhdEU6gVYzA"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import shutil\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "from datasets import load_dataset\n",
        "from evaluate import load as metrics_loader\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "from transformers import Seq2SeqTrainer\n",
        "from transformers import WhisperForConditionalGeneration\n",
        "from transformers import WhisperProcessor\n",
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hznHHF7ameeQ"
      },
      "source": [
        "## Function definition\n",
        "\n",
        "This section defines the Word Error Rate (WER) calculation function, which is used to evaluate the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "en_N9eNyP1J3"
      },
      "outputs": [],
      "source": [
        "wer_metric = metrics_loader(\"wer\")\n",
        "transcript_normalizer = BasicTextNormalizer()\n",
        "\n",
        "def get_wer(references, predictions, normalize=True, verbose=True):\n",
        "  rs = references\n",
        "  ps = predictions\n",
        "  if normalize:\n",
        "    ps = [transcript_normalizer(x) for x in predictions]\n",
        "    rs = [transcript_normalizer(x) for x in references]\n",
        "  if verbose:\n",
        "    for r, p in zip(rs, ps):\n",
        "      print(r)\n",
        "      print(p)\n",
        "      print()\n",
        "\n",
        "  return wer_metric.compute(references=rs, predictions=ps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tepVveUxmeeQ"
      },
      "source": [
        "This function counts the number of trainable parameters in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHG6SZz6D6QO"
      },
      "outputs": [],
      "source": [
        "def count_trainable_parameters(model):\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "    return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrZDdgRkEMOK"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "This cell downloads the audio data and transcriptions from your Firebase storage bucket to the Colab environment. Replace <YOUR FIREBASE PROJECT NAME> with your actual project ID. This is the same ID as you chose when you created your Firebase project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARwsVVmwC8Gg"
      },
      "outputs": [],
      "source": [
        "AUDIO_DATA_DIR = '/content/asr_data'\n",
        "FIREBASE_PROJECT = '<YOUR FIREBASE PROJECT NAME>'\n",
        "!mkdir -p {AUDIO_DATA_DIR}\n",
        "\n",
        "# download all for now from firebase storage\n",
        "!gsutil -m cp -r gs://{FIREBASE_PROJECT}.appspot.com/data/ {AUDIO_DATA_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBUxCkvTEQzK"
      },
      "source": [
        "## Prepare Dataset\n",
        "\n",
        "This cell prepares the dataset by splitting the data into training, testing, and development sets, creating a metadata CSV file, and loading the data into a [Hugging Face Dataset](https://huggingface.co/blog/audio-datasets) object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRVvdjM6ERzl"
      },
      "outputs": [],
      "source": [
        "#@title Create dataset\n",
        "AUDIO_FOLDER_DIR = '/content/audio_folder'\n",
        "!mkdir {AUDIO_FOLDER_DIR}\n",
        "\n",
        "DEV_METADATA_FILE = os.path.join(AUDIO_FOLDER_DIR, 'dev', 'metadata.csv')\n",
        "TEST_METADATA_FILE = os.path.join(AUDIO_FOLDER_DIR, 'test', 'metadata.csv')\n",
        "TRAIN_METADATA_FILE = os.path.join(AUDIO_FOLDER_DIR, 'train', 'metadata.csv')\n",
        "\n",
        "csv_file_map = {\n",
        "    'dev': DEV_METADATA_FILE,\n",
        "    'test': TEST_METADATA_FILE,\n",
        "    'train': TRAIN_METADATA_FILE\n",
        "}\n",
        "\n",
        "# percentage train/test/dev\n",
        "#@markdown train and test together cannot exceed 90% of the data (at least 10% used as dev set)\n",
        "TRAIN_PORTION = 0.8 #@param{type: 'number'}\n",
        "TEST_PORTION = 0.1 #@param{type: 'number'}\n",
        "\n",
        "verbose = False #@param{type: 'boolean'}\n",
        "\n",
        "assert (TRAIN_PORTION + TEST_PORTION <= 0.9)\n",
        "\n",
        "\n",
        "# get sizes\n",
        "num_audios = len(os.listdir(os.path.join(AUDIO_DATA_DIR, 'data')))\n",
        "train_offset = int(num_audios * TRAIN_PORTION)\n",
        "test_offset = train_offset + int(num_audios * TEST_PORTION)\n",
        "\n",
        "# copy audios and create metadata\n",
        "!mkdir -p {AUDIO_FOLDER_DIR}/train\n",
        "!mkdir -p {AUDIO_FOLDER_DIR}/dev\n",
        "!mkdir -p {AUDIO_FOLDER_DIR}/test\n",
        "\n",
        "\n",
        "for i in csv_file_map.keys():\n",
        "  f = open(csv_file_map[i], 'w', newline='')\n",
        "  spamwriter = csv.writer(f)\n",
        "  spamwriter.writerow(['file_name', 'transcription'])\n",
        "  f.close()\n",
        "\n",
        "for i in range(0, num_audios):\n",
        "  current_split = ''\n",
        "  if i < train_offset:\n",
        "    current_split = 'train'\n",
        "  elif i < test_offset:\n",
        "    current_split = 'test'\n",
        "  else:\n",
        "    current_split = 'dev'\n",
        "\n",
        "  f = open(csv_file_map[current_split], 'a', newline='')\n",
        "  spamwriter = csv.writer(f)\n",
        "\n",
        "  orig_audio_file = os.path.join(AUDIO_DATA_DIR, 'data', str(i), 'recording.wav')\n",
        "  transcript_file = os.path.join(AUDIO_DATA_DIR, 'data', str(i), 'phrase.txt')\n",
        "  target_audio_file = os.path.join(AUDIO_FOLDER_DIR, current_split, 'recording_' + str(i) + '.wav')\n",
        "  relative_target_audio_file = os.path.join('recording_' + str(i) + '.wav')\n",
        "  transcript = open(transcript_file, 'r').read()\n",
        "\n",
        "  if verbose:\n",
        "    print(orig_audio_file + '-->' + target_audio_file + '\\t' + relative_target_audio_file + '\\t' + transcript)\n",
        "\n",
        "  shutil.copyfile(orig_audio_file, target_audio_file)\n",
        "  spamwriter.writerow([relative_target_audio_file, transcript])\n",
        "  f.close()\n",
        "\n",
        "# create huggingface dataset\n",
        "my_audio_dataset = load_dataset(\"audiofolder\", data_dir=AUDIO_FOLDER_DIR, streaming=False)\n",
        "print(my_audio_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpzjopyqmeeR"
      },
      "outputs": [],
      "source": [
        "# we can now inspect examples in the dataset\n",
        "my_audio_dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYjs6O3mFzSj"
      },
      "source": [
        "## Model Training\n",
        "This cell uses an interactive dropdown widget to select the language for the ASR model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kycBlJZcRSZu"
      },
      "outputs": [],
      "source": [
        "#@title Select ASR model language\n",
        "#@markdown Run cell to get list of languages supported by model. Then select the language.\n",
        "#@markdown Don't re-run cell afterwards, as this will reset language selection.\n",
        "from transformers.models.whisper import tokenization_whisper\n",
        "\n",
        "languages_list = list(tokenization_whisper.TO_LANGUAGE_CODE.values())\n",
        "language_picker = widgets.Dropdown(options=languages_list, value='en')\n",
        "print('Language:')\n",
        "language_picker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz9KPQluGKj0"
      },
      "outputs": [],
      "source": [
        "WHISPER_MODEL_TYPE = \"openai/whisper-small\" #@param['openai/whisper-tiny.en', 'openai/whisper-tiny', 'openai/whisper-base', 'openai/whisper-small', 'openai/whisper-medium', 'openai/whisper-large', 'openai/whisper-large-v3', 'openai/whisper-large-v3-turbo']{}\n",
        "LANGUAGE = language_picker.value\n",
        "TASK = \"transcribe\"\n",
        "\n",
        "print('Using Language: ', LANGUAGE)\n",
        "print('Using model:', WHISPER_MODEL_TYPE)\n",
        "processor = WhisperProcessor.from_pretrained(WHISPER_MODEL_TYPE, language=LANGUAGE, task=TASK)\n",
        "base_model = WhisperForConditionalGeneration.from_pretrained(WHISPER_MODEL_TYPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izEU-ipfFvdk"
      },
      "source": [
        "## Extract features on dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odG6QE_ew8cN"
      },
      "outputs": [],
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('device is: ', device)\n",
        "\n",
        "# for more efficient dataset processing\n",
        "torch.set_num_threads(1)\n",
        "torch.get_num_threads()\n",
        "num_proc = os.cpu_count()\n",
        "print('# processors:', num_proc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZIZS6o1D6EY"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "def prepare_dataset(batch):\n",
        "    audio = batch[\"audio\"]\n",
        "    batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "    batch[\"labels\"] = processor.tokenizer(batch[\"transcription\"]).input_ids\n",
        "    batch[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
        "    return batch\n",
        "\n",
        "\n",
        "my_audio_dataset = my_audio_dataset.map(prepare_dataset,\n",
        "                                        writer_batch_size=1,\n",
        "                                        num_proc=num_proc,\n",
        "                                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROxRB8hEbQXF"
      },
      "source": [
        "## Configure Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK9ZaQf0I9iu"
      },
      "outputs": [],
      "source": [
        "#@title Training Hyper Parameters\n",
        "OUTPUT_DIR = '/content/whisper_tuning' #@param\n",
        "LOG_DIR = os.path.join(OUTPUT_DIR, 'logs')\n",
        "\n",
        "LEARNING_RATE = 1e-5 #@param\n",
        "BATCH_SIZE = 8 #@param\n",
        "MAX_EPOCHS = 10 #@param\n",
        "WARMUP_STEPS = 10 #@param\n",
        "# set this as short as possible for your data\n",
        "MAX_GEN_LEN = 32 #@param\n",
        "# if save steps is 0, only last and best model will be written\n",
        "SAVE_STEPS = 0 #@param\n",
        "\n",
        "# see\n",
        "# https://huggingface.co/docs/transformers/v4.46.2/en/main_classes/trainer#transformers.TrainingArguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    logging_dir=OUTPUT_DIR + '/logs',\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=1,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    num_train_epochs=MAX_EPOCHS,\n",
        "    #\n",
        "    lr_scheduler_type='constant_with_warmup',\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    #\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=MAX_GEN_LEN,\n",
        "    eval_steps=5,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    #\n",
        "    save_steps=SAVE_STEPS,\n",
        "    logging_steps=1,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    #\n",
        "    push_to_hub=False,\n",
        "    remove_unused_columns=False,\n",
        "    eval_on_start=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWoQHhnZp52r"
      },
      "outputs": [],
      "source": [
        "#@title Ensure we set the language for training\n",
        "\n",
        "base_model.generation_config.language = LANGUAGE\n",
        "base_model.generation_config.task = TASK\n",
        "base_model.generation_config.forced_decoder_ids = None\n",
        "base_model.config.forced_decoder_ids = None\n",
        "\n",
        "# to use gradient checkpointing\n",
        "base_model.config.use_cache = False\n",
        "\n",
        "print('language set to:', base_model.generation_config.language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JudbJXhQXZ-d"
      },
      "outputs": [],
      "source": [
        "#@title define which parameters to update\n",
        "#@markdown For personalizartion, we typically only want to update the encoder and projection layer.\n",
        "#@markdown Updating the decoder layer may lead to overfitting.\n",
        "UPDATE_ENCODER = True #@param{type: 'boolean'}\n",
        "UPDATE_DECODER = False #@param{type: 'boolean'}\n",
        "UPDATE_PROJ = True #@param{type: 'boolean'}\n",
        "base_model.model.encoder.requires_grad_(UPDATE_ENCODER)\n",
        "base_model.model.decoder.requires_grad_(UPDATE_DECODER)\n",
        "base_model.proj_out.requires_grad_(UPDATE_PROJ)\n",
        "\n",
        "\n",
        "print('encoder params to update/total:', count_trainable_parameters(base_model.model.encoder), base_model.model.encoder.num_parameters())\n",
        "print('decoder parans to update/total:', count_trainable_parameters(base_model.model.decoder), base_model.model.decoder.num_parameters())\n",
        "\n",
        "print('overall # trainable parameters:', count_trainable_parameters(base_model))\n",
        "print('.   overall # model parameters:', base_model.model.num_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMSgzDqUJOI0"
      },
      "outputs": [],
      "source": [
        "#@title Define Trainer\n",
        "import evaluate\n",
        "metric = evaluate.load(\"wer\")\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "    decoder_start_token_id: int\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
        "    processor=processor,\n",
        "    decoder_start_token_id=base_model.config.decoder_start_token_id,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=base_model,\n",
        "    train_dataset=my_audio_dataset[\"train\"],\n",
        "    eval_dataset=my_audio_dataset[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH76s8LRbWXo"
      },
      "source": [
        "## Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XrmH1hRkPs7"
      },
      "outputs": [],
      "source": [
        "# start tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {LOG_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SOcObNYJp0h"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7AgP22QmeeV"
      },
      "outputs": [],
      "source": [
        "print('evaluating best model after fine-tuning, lanuage:', LANGUAGE)\n",
        "trainer.evaluate(my_audio_dataset[\"validation\"], language=LANGUAGE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFcH724AJ5gZ"
      },
      "outputs": [],
      "source": [
        "#@title Safe trained model\n",
        "\n",
        "#@markdown It is recommended to save the final model in Google Drive as it is much faster to download it from there than from Colab (especially true for large models).\n",
        "\n",
        "save_in_drive = False #@param {type: 'boolean'}\n",
        "\n",
        "model_output_dir_name = 'finetuned_whisper_model' #@param {type: 'string'}\n",
        "\n",
        "\n",
        "if save_in_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  output_dir = os.path.join('/content/drive/MyDrive/', model_output_dir_name)\n",
        "else:\n",
        "  output_dir = os.path.join('/content/', model_output_dir_name)\n",
        "\n",
        "!mkdir -p {output_dir}\n",
        "\n",
        "print('Saving model in:', output_dir)\n",
        "\n",
        "# save model and processor, so we can later load as pretrained\n",
        "save_model_dir = os.path.join(output_dir, 'saved_model')\n",
        "trainer.model.save_pretrainedl(save_model_dir, safe_serialization=False)\n",
        "\n",
        "# save processor also\n",
        "save_processor_dir = os.path.join(output_dir, 'saved_processor')\n",
        "processor.save_pretrained(save_processor_dir, safe_serialization=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASKNndl6Kk19"
      },
      "source": [
        "# Test adapted model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfF9b3m_Kl7s"
      },
      "outputs": [],
      "source": [
        "def transcribe_from_dataset(dataset_sample, whisper_model, max_new_tokens=128):\n",
        "  input_features = processor.feature_extractor(\n",
        "    dataset_sample[\"array\"],\n",
        "    sampling_rate=dataset_sample[\"sampling_rate\"],\n",
        "    return_tensors=\"pt\").input_features\n",
        "\n",
        "  predicted_ids = whisper_model.generate(\n",
        "      input_features, max_new_tokens=max_new_tokens,\n",
        "      language=LANGUAGE, task=TASK, forced_decoder_ids=None)\n",
        "  transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
        "  return transcription[0].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtZNfclYKnt5"
      },
      "outputs": [],
      "source": [
        "#@title Get WER on default and tuned model for comparison\n",
        "\n",
        "default_model = WhisperForConditionalGeneration.from_pretrained(WHISPER_MODEL_TYPE, local_files_only=True)\n",
        "finetuned_model = WhisperForConditionalGeneration.from_pretrained(save_pretrained_model_dir, local_files_only=True)\n",
        "\n",
        "num_test_samples = 10 #@param{type: 'number'}\n",
        "normalize_for_wer_calc = True #@param{type: 'boolean'}\n",
        "\n",
        "num_test_samples = min(num_test_samples, len(my_audio_dataset['test']))\n",
        "print('number of test examples to process:', num_test_samples)\n",
        "\n",
        "predictions = []\n",
        "finetuned_predictions = []\n",
        "references  = []\n",
        "\n",
        "for idx in range(num_test_samples):\n",
        "  print('inference on example:', idx)\n",
        "  sample = my_audio_dataset['test'][idx][\"audio\"]\n",
        "  predictions.append(transcribe_from_dataset(sample, default_model))\n",
        "  finetuned_predictions.append(transcribe_from_dataset(sample, finetuned_model))\n",
        "  references.append(my_audio_dataset['test'][idx]['transcription'])\n",
        "\n",
        "default_wer = get_wer(references=references, predictions=predictions, normalize=normalize_for_wer_calc, verbose=False)\n",
        "finetuned_wer = get_wer(references=references, predictions=finetuned_predictions, normalize=normalize_for_wer_calc, verbose=False)\n",
        "\n",
        "print(f'DEFAULT WER: {default_wer}')\n",
        "print(f'FINETUNED WER: {finetuned_wer}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkaE3xn1ODn5"
      },
      "outputs": [],
      "source": [
        "#@title Run inference on individual example of test set\n",
        "%%time\n",
        "test_set_idx = 5 #@param\n",
        "sample = my_audio_dataset['test'][test_set_idx][\"audio\"]\n",
        "transcript = my_audio_dataset['test'][test_set_idx][\"transcription\"]\n",
        "# use default_model or finetuned_model\n",
        "model = finetuned_model\n",
        "pred = transcribe_from_dataset(sample, model, max_new_tokens=32)\n",
        "print('Ground truth: ', transcript)\n",
        "print('  Prediction: ', pred)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "I9e4g78MdQDd"
      ],
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}